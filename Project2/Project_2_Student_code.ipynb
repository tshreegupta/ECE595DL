{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2_Student_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tshreegupta/ECE595DL/blob/main/Project2/Project_2_Student_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR4A9aGPpiuZ"
      },
      "source": [
        "#Student Name: \n",
        "#ECE 595 Machine Learning II\n",
        "#Project 2: Autoencoders - Student Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viCY7Zn5psMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a6e382-1358-40fa-9b01-0f7a83ffc56a"
      },
      "source": [
        "#Import necessary packages\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vATVUR9pxNT"
      },
      "source": [
        "#Part 0: Importing and Normalizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN1cUmn1p0vn"
      },
      "source": [
        "#Load MNIST data and normalize to [0,1]\n",
        "(data_train, _), (data_test, _) = mnist.load_data()\n",
        "data_train = data_train/255.0\n",
        "data_test = data_test/255.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKY7C6Ybmg31",
        "outputId": "a05cca2f-f011-4166-c2cf-8c80419a084d"
      },
      "source": [
        "print(data_train.shape[0],data_train.shape[1]*data_train.shape[2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beNpVSplNsaJ"
      },
      "source": [
        "#Part 1: Deep Fully-Connected AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfm3fYwcOyBk"
      },
      "source": [
        "Answer the following questions: \n",
        "\n",
        "\n",
        "1.  We want to predict output values of the intensity of the pixels which are between 0 to 1. Therefore choice of output layer activation function is important. (Open ended reasoning question). (a) Choose 'softmax’ or ‘sigmoid’. Reason why one is preferred over the other.\n",
        "Ans. The autoencoder output is the reconstruction of input image so, each output neuron will represent a pixel in the image. Therefore, we will use softmax activation function as the value of outpur should be between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqYvEhZHN_h4"
      },
      "source": [
        "#Reshape training and testing data into 784-dimensional vectors\n",
        "#FILL IN THIS CODE BLOCK\n",
        "data_train_reshape = data_train.reshape(data_train.shape[0],data_train.shape[1]*data_train.shape[2])\n",
        "data_test_reshape = data_test.reshape(data_test.shape[0],data_test.shape[1]*data_test.shape[2])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n0UnyynyqNg",
        "outputId": "6e7f46a8-cbd6-4446-d835-fd8d3b5d3870"
      },
      "source": [
        "print(data_test_reshape.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEwdV09ms_KF"
      },
      "source": [
        "#Create autoencoder architecture\n",
        "def deep_ae():\n",
        "    model = Sequential()\n",
        "\n",
        "    #FILL THIS IN WITH MODEL ARCHITECTURE\n",
        "    # Layer1: First Hidden Layer\n",
        "    model.add(Dense(400,activation='softmax',use_bias= True,kernel_initializer='normal', input_dim = 784))\n",
        "    \n",
        "    # Layer2: Second Hidden Layer\n",
        "    model.add(Dense(200,activation='softmax',use_bias= True,kernel_initializer='normal'))\n",
        "    \n",
        "    # Layer3: Third Hidden Layer\n",
        "    model.add(Dense(100,activation='softmax',use_bias= True,kernel_initializer='normal'))\n",
        "\n",
        "    # Layer4: Fourth Hidden Layer\n",
        "    model.add(Dense(200,activation='softmax',use_bias= True,kernel_initializer='normal'))\n",
        "\n",
        "    # Layer5: Fifth Hidden Layer\n",
        "    model.add(Dense(400,activation='softmax',use_bias= True,kernel_initializer='normal'))\n",
        "\n",
        "    # Layer6: Output Layer\n",
        "    model.add(Dense(784,activation='softmax',use_bias= True,kernel_initializer='normal'))\n",
        "    return model\n",
        "\n",
        "#Create deep autoencoder graph\n",
        "deep_ae = deep_ae()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEsZHS6EtlaD",
        "outputId": "0277ae5d-912b-4dd3-b349-a4207502dbb6"
      },
      "source": [
        "#Compile model using an appropriate loss and optimizer algorithm\n",
        "#FILL THIS IN\n",
        "#Part1: mean_squared_error\n",
        "deep_ae.compile(loss= 'mean_squared_error',\n",
        "                    optimizer= 'adam',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "#Train the model and assign training meta-data to a variable\n",
        "#FILL THIS IN\n",
        "deep_ae_meta_mse = deep_ae.fit(data_train_reshape,data_train_reshape,\n",
        "                           batch_size=1024,\n",
        "                           epochs=150,\n",
        "                           shuffle=True,\n",
        "                           validation_data=(data_test_reshape,data_test_reshape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/150\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.1117 - accuracy: 0.0017 - val_loss: 0.1136 - val_accuracy: 0.0049\n",
            "Epoch 2/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1117 - accuracy: 0.0068 - val_loss: 0.1136 - val_accuracy: 0.0083\n",
            "Epoch 3/150\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1116 - accuracy: 0.0090 - val_loss: 0.1136 - val_accuracy: 0.0083\n",
            "Epoch 4/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1116 - accuracy: 0.0090 - val_loss: 0.1136 - val_accuracy: 0.0083\n",
            "Epoch 5/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1116 - accuracy: 0.0090 - val_loss: 0.1135 - val_accuracy: 0.0083\n",
            "Epoch 6/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1116 - accuracy: 0.0090 - val_loss: 0.1135 - val_accuracy: 0.0083\n",
            "Epoch 7/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1116 - accuracy: 0.0090 - val_loss: 0.1135 - val_accuracy: 0.0083\n",
            "Epoch 8/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1115 - accuracy: 0.0090 - val_loss: 0.1135 - val_accuracy: 0.0083\n",
            "Epoch 9/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1115 - accuracy: 0.0090 - val_loss: 0.1135 - val_accuracy: 0.0083\n",
            "Epoch 10/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1115 - accuracy: 0.0072 - val_loss: 0.1134 - val_accuracy: 0.0037\n",
            "Epoch 11/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1115 - accuracy: 0.0081 - val_loss: 0.1134 - val_accuracy: 0.0037\n",
            "Epoch 12/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1115 - accuracy: 0.0044 - val_loss: 0.1134 - val_accuracy: 0.0161\n",
            "Epoch 13/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1114 - accuracy: 0.0150 - val_loss: 0.1134 - val_accuracy: 0.0161\n",
            "Epoch 14/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1114 - accuracy: 0.0150 - val_loss: 0.1133 - val_accuracy: 0.0161\n",
            "Epoch 15/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1114 - accuracy: 0.0150 - val_loss: 0.1133 - val_accuracy: 0.0161\n",
            "Epoch 16/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1114 - accuracy: 0.0150 - val_loss: 0.1133 - val_accuracy: 0.0161\n",
            "Epoch 17/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1114 - accuracy: 0.0150 - val_loss: 0.1133 - val_accuracy: 0.0161\n",
            "Epoch 18/150\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.1113 - accuracy: 0.0150 - val_loss: 0.1133 - val_accuracy: 0.0161\n",
            "Epoch 19/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1113 - accuracy: 0.0150 - val_loss: 0.1132 - val_accuracy: 0.0161\n",
            "Epoch 20/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1113 - accuracy: 0.0150 - val_loss: 0.1132 - val_accuracy: 0.0161\n",
            "Epoch 21/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1113 - accuracy: 0.0150 - val_loss: 0.1132 - val_accuracy: 0.0161\n",
            "Epoch 22/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1113 - accuracy: 0.0150 - val_loss: 0.1132 - val_accuracy: 0.0161\n",
            "Epoch 23/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1112 - accuracy: 0.0150 - val_loss: 0.1132 - val_accuracy: 0.0161\n",
            "Epoch 24/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1112 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 25/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1112 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 26/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1112 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 27/150\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.1112 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 28/150\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 29/150\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1131 - val_accuracy: 0.0161\n",
            "Epoch 30/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 31/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 32/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 33/150\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 34/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 35/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1111 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 36/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 37/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1130 - val_accuracy: 0.0161\n",
            "Epoch 38/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 39/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 40/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 41/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 42/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 43/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 44/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1110 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 45/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 46/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 47/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 48/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1129 - val_accuracy: 0.0161\n",
            "Epoch 49/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 50/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 51/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 52/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 53/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 54/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 55/150\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 56/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 57/150\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 58/150\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 59/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1109 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 60/150\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.1108 - accuracy: 0.0150 - val_loss: 0.1128 - val_accuracy: 0.0161\n",
            "Epoch 61/150\n",
            " 4096/60000 [=>............................] - ETA: 6s - loss: 0.1110 - accuracy: 0.0154"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo7qsfgpxpcU"
      },
      "source": [
        "#Calculate the reconstructions of the testing set (output of autoencoder on test set)\n",
        "#FILL THIS IN\n",
        "reconstruction_mse = deep_ae.predict(data_test)\n",
        "\n",
        "#Obtain encoder representation of data\n",
        "get_hl = K.function([deep_ae.layers[0].input], [deep_ae.layers[2].output]) #The third hidden layer is indexed at 2\n",
        "deep_ae_hl = get_hl([data_test_reshape_fcae])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO8k7t8XuwOX"
      },
      "source": [
        "#Compile model using an appropriate loss and optimizer algorithm\n",
        "#FILL THIS IN\n",
        "#Part1: binary_crossentropy\n",
        "deep_ae.compile(loss= 'binary_crossentropy',\n",
        "                    optimizer= 'adam',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "#Train the model and assign training meta-data to a variable\n",
        "#FILL THIS IN\n",
        "deep_ae_meta_bc = deep_ae.fit(data_train,data_train,\n",
        "                           batch_size=1024,\n",
        "                           epochs=150,\n",
        "                           shuffle=True,\n",
        "                           validation_data=(data_test,data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCbr6wWPxtwg"
      },
      "source": [
        "#Calculate the reconstructions of the testing set (output of autoencoder on test set)\n",
        "#FILL THIS IN\n",
        "reconstruction_bc = deep_ae.predict(data_test)\n",
        "\n",
        "\n",
        "#Obtain encoder representation of data\n",
        "get_hl = K.function([deep_ae.layers[0].input], [deep_ae.layers[2].output]) #The third hidden layer is indexed at 2\n",
        "deep_ae_hl = get_hl([data_test_reshape_fcae])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1psOUHtZpk"
      },
      "source": [
        "#Plot loss vs epoch for BCE and MSE [Together or separate, Both accepted]\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE PLOT\n",
        "plt.plot(deep_ae_meta_mse.history['loss'])\n",
        "plt.plot(deep_ae_meta_bc.history['loss'])\n",
        "plt.title('BCE vs. MSE ')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['MSE','BCE'],loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ALTu2Vte2-"
      },
      "source": [
        "#Show samples of 10 images, their hidden layer representations, and their reconstructions\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE SAMPLES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3piQQZReeW"
      },
      "source": [
        "Answer the following questions: \n",
        "\n",
        "\n",
        "1.  Question 2c: BONUS [reasoning question]: Pair the output layer activation and loss function which performs better together. Linear, sigmoid, Binary cross entropy and Mean Square error\n",
        " \n",
        "\n",
        "2.  Question 5: Which loss function is better and why?\n",
        "\n",
        "3.  Question 6: If we were to predict pixels values [0 to 255] directly at the output of last layer. Should there be an activation function in last later?  If yes, which activation function and why?,  If No, reason why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4O3FwRt-sO"
      },
      "source": [
        "#Part 2: Deep Convolutional AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a7Xlz7VuBl8"
      },
      "source": [
        "#Reshape data into 2-D signals and account for grayscale channel in each image\n",
        "#FILL IN THIS CODE BLOCK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-wENWWnuQGa"
      },
      "source": [
        "#Create Convolutional AutoEncoder Architecture\n",
        "def cae():\n",
        "    model = Sequential()\n",
        "\n",
        "    #FILL THIS IN WITH MODEL ARCHITECTURE\n",
        "\n",
        "    return model\n",
        "\n",
        "#Create deep autoencoder graph\n",
        "conv_ae = cae()\n",
        "\n",
        "#Compile model using an appropriate loss and optimizer algorithm\n",
        "#FILL THIS IN\n",
        "\n",
        "#Train the model and assign training meta-data to a variable\n",
        "#FILL THIS IN\n",
        "\n",
        "#Calculate the reconstructions of the testing set (output of autoencoder on test set)\n",
        "#FILL THIS IN\n",
        "\n",
        "\n",
        "#Obtain encoder representation of data\n",
        "get_hl = K.function([conv_ae.layers[0].input], [conv_ae.layers[3].output])  # The fourth hidden layer are indexed at 3\n",
        "conv_ae_hl = get_hl([data_test_reshape_cae])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sVJO4y1ujCZ"
      },
      "source": [
        "#Plot loss vs epoch  for BCE and MSE [Together or separate, Both accepted]\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE PLOT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcclERBAu0-u"
      },
      "source": [
        "#Show samples of 10 images, their hidden layer representations, and their reconstructions\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE SAMPLES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8lacfJnUyET"
      },
      "source": [
        "Answer the following questions: \n",
        "\n",
        "\n",
        "1.  Question 4: Which loss function is better and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QfcpOPXvezs"
      },
      "source": [
        "#Part 3: Denoising AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lVJ3pfvhT5"
      },
      "source": [
        "#Inject noise into testing data\n",
        "noise_factor = 0.25\n",
        "data_train_noisy = data_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data_train.shape)\n",
        "data_test_noisy = data_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data_test.shape)\n",
        "\n",
        "#Clip to stay within valid (normalized) pixel range\n",
        "data_train_noisy = np.clip(data_train_noisy, 0., 1.)\n",
        "data_test_noisy = np.clip(data_test_noisy, 0., 1.)\n",
        "\n",
        "#Reshape data to comply with input of denoising autoencoder\n",
        "#FILL THIS IN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4zyjNfawzpU"
      },
      "source": [
        "#Show samples of 10 original images and their corrsponding noisy counterparts from the training set\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE SAMPLES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nuk3UxSxD9D"
      },
      "source": [
        "#Create denoising autoencoder architecture\n",
        "def dae():\n",
        "\n",
        "    #FILL THIS IN WITH MODEL ARCHITECTURE\n",
        "\n",
        "    return model\n",
        "  \n",
        "\n",
        "  \n",
        "#Compile and train the DAE\n",
        "#FILL THIS IN\n",
        "\n",
        "\n",
        "#Generate denoised versions of noisy inputs\n",
        "#FILL THIS IN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_azeum1yxYIf"
      },
      "source": [
        "#Plot loss vs epoch\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE PLOT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-Xh7EDx0Vq"
      },
      "source": [
        "#Show samples of 10 original images, their noisy counterparts, and their de-noised images from the testing set\n",
        "#FILL THIS CODE BLOCK IN AND PRODUCE SAMPLES"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}